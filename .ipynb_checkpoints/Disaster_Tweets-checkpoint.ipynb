{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#7B241C\"> Natural Language Processing</h1>\n",
    "<h2 style=\"color:#922B21\"> “Disaster Tweets”</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img1.jpg\" width=600px >\n",
    "\n",
    "<h2 style=\"color:#641E16;\">Natural language processing </h2>\n",
    "<ul>\n",
    "    <li>Field of study, focused on making sense of language Using statistics and computers</li>\n",
    "    <li>helps computers communicate with humans in their language and scales other language-related tasks. For example, NLP   makes it possible for computers to read text, hear speech, interpret it, measure sentiment, and determine which parts are important.</li>\n",
    "</ul>\n",
    "<h3 style=\"color:#922B21;\">NLP applications include: </h3>\n",
    "<ul>\n",
    "<li>Chatbots </li>\n",
    "<li>Translation </li>\n",
    "<li>Sentiment analysis</li>\n",
    "<li>...and many more!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#2980B9; font-size:35px\">Twitter </h2>\n",
    "<img src=\"tweet.jpg\" width=500px style=\"border-radius:30%\">\n",
    "\n",
    "A lot of people now are using smartphones, which enables people to easily be on social media.\n",
    "\n",
    "Twitter uniquely provides its users the opportunity to discover what's happening in the world. \n",
    "\n",
    "According to Twitter’s latest figures from the fourth quarter of 2020, the platform boasts <a href=\"https://s22.q4cdn.com/826641620/files/doc_financials/2020/q4/FINAL-Q4'20-TWTR-Shareholder-Letter.pdf\"> 192 million daily active users </a>(Twitter, 2021). \n",
    "\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programmatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "\n",
    "But, it’s not always clear whether a person’s words are announcing a disaster, Take this example:<br>\n",
    "<img src=\"tweet_screenshot.png\" width=250px style=\"float:left\"><br>\n",
    "<pre>The author explicitly uses the word “ABLAZE” but means it metaphorically.\n",
    "This is clear to a human right away, especially with the visual aid.\n",
    "But it’s less clear to a machine.</pre>\n",
    "<pre>This tweet can be found at <a href=\"https://twitter.com/AnyOtherAnnaK/status/629195955506708480\">tweet source</a></pre> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#1F618D;font-size:35px\">Disaster Tweets<h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px;\">In this project, I built a machine learning model that predicts which Tweets are about real disasters and which ones aren’t.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Files:</h3>\n",
    "<ul>\n",
    "<li>train.csv - the training set</li>\n",
    "<li>test.csv - the test set</li>\n",
    "<li>sample_submission.csv - a sample submission file in the correct format</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Columns:</h3>\n",
    "<ul>\n",
    "<li>id - a unique identifier for each tweet</li>\n",
    "<li>text - the text of the tweet</li>\n",
    "<li>location - the location the tweet was sent from (may be blank)</li>\n",
    "<li>keyword - a particular keyword from the tweet (may be blank)</li>\n",
    "<li>target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What am I predicting?</h3><br>\n",
    "Predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data</h3><br>\n",
    "This dataset was created by the company figure-eight and originally shared on their <a href=\"https://appen.com/open-source-datasets/\">‘Data For Everyone’</a> website here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> So, Let's get started</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing Dependencies and Data</h3>\n",
    "<pre>Start by importing the dependencies and the data.\n",
    "The data is stored as a comma-separated values (CSV)  file, so I will use pandas’ read_csv() function to open it into \n",
    "a DataFrame.</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from csv\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_sample = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample[\"target\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exploratory Data Analysis(EDA)</h3>\n",
    "<pre>How do we get from data to answers? \n",
    "Exploratory data analysis is a process for exploring datasets, answering questions, \n",
    "and visualizing results.</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first 5 rows of the train data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first 5 rows of the test data\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the data\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any duplicated values in the train data \n",
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any duplicated values in the test data \n",
    "df_test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the columns' names of the train data \n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the columns' names of the test data \n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                   104\n",
       "New York               71\n",
       "United States          50\n",
       "London                 45\n",
       "Canada                 29\n",
       "                     ... \n",
       "The Desert              1\n",
       "Mogadishu, Somalia      1\n",
       "Jerusalem!              1\n",
       "OK                      1\n",
       "cleveland, oh           1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the locations in the df_train  \n",
    "df_train[\"location\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "New York                 38\n",
       "USA                      37\n",
       "Worldwide                16\n",
       "United States            15\n",
       "Canada                   13\n",
       "                         ..\n",
       "Wayne, NJ                 1\n",
       "Paonia, Colorado          1\n",
       "Crescent Moon w/ Wook     1\n",
       "Mystic Falls              1\n",
       "Fukushima, JAPAN          1\n",
       "Name: location, Length: 1602, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the locations in the df_test  \n",
    "df_test[\"location\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "armageddon               42\n",
       "deluge                   42\n",
       "sinking                  41\n",
       "harm                     41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the keywords in the df_train  \n",
    "df_train[\"keyword\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deluged          23\n",
       "rubble           22\n",
       "demolished       22\n",
       "obliteration     21\n",
       "annihilation     21\n",
       "                 ..\n",
       "forest%20fire     5\n",
       "threat            5\n",
       "fatalities        5\n",
       "inundation        4\n",
       "epicentre         1\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the keywords in the df_test  \n",
    "df_test[\"keyword\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unimportant features from train data\n",
    "df_train.drop([\"keyword\",\"location\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unimportant features from test data\n",
    "df_test.drop([\"keyword\",\"location\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "text      0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disaster_Tweets_numbers: 3271\n",
      "not Disaster_Tweets_numbers: 4342\n"
     ]
    }
   ],
   "source": [
    "# Number of Disaster Tweets\n",
    "print(\"Disaster_Tweets_numbers: \" +str(len(df_train[df_train[\"target\"]==1])))\n",
    "print(\"not Disaster_Tweets_numbers: \" +str(len(df_train[df_train[\"target\"]==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is data cleaning and why is it important?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning, or data cleansing, is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset.\n",
    "To make our machine learning model and predict the disaster tweets, we cannot go straight from raw text to fitting the model  it is nonsense to convert unnersesary words, characters, numbers, and punctuations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Filter Out Punctuation and numbers, and tokenize the text\n",
    "Tokenization is the process by which we break down articles into individual sentences or words, as needed. Besides the tokenization method provided by NLTK, we might have to perform additional filtration to remove tokens that are entirely numeric values or punctuation.\n",
    "So, here I use RegexpTokenizer, which will get us text which is not words(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Numbers and tokenize the text into words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "df_train['cleaned'] = [tokenizer.tokenize(item) for item in df_train['text']]\n",
    "df_test['cleaned'] = [tokenizer.tokenize(item) for item in df_test['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Our, Deeds, are, the, Reason, of, this, earth...\n",
       "1           [Forest, fire, near, La, Ronge, Sask, Canada]\n",
       "2       [All, residents, asked, to, shelter, in, place...\n",
       "3       [people, receive, wildfires, evacuation, order...\n",
       "4       [Just, got, sent, this, photo, from, Ruby, Ala...\n",
       "                              ...                        \n",
       "7608    [Two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [aria_ahrary, TheTawniest, The, out, of, contr...\n",
       "7610    [M, UTC, km, S, of, Volcano, Hawaii, http, t, ...\n",
       "7611    [Police, investigating, after, an, e, bike, co...\n",
       "7612    [The, Latest, More, Homes, Razed, by, Northern...\n",
       "Name: cleaned, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Stemming\n",
    "\n",
    "Stemming is the process by which we bring down a word from its different forms to the root word.<br>\n",
    "This helps us establish meaning to different forms of the same words without having to deal with each form separately. <br>\n",
    "For example, the words 'fishing', 'fished', and 'fisher' all get stemmed to the word 'fish'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Language\n",
    "# from langdetect import detect\n",
    "# df_train[\"lang\"] = [detect( i ) for i in df_train[\"text\"]]\n",
    "# df_train[\"lang\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"lang\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stemming using WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Create an English language SnowballStemmer object\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df_train['cleaned'] = df_train['cleaned'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "df_test['cleaned'] = df_test['cleaned'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Delete stop words\n",
    "\n",
    "Stop words are commonly used words (such as “the”, “a”, “an”, “in”)  \n",
    "This words takes space in the dataset, so we are going to delete it.\n",
    "For this, we can remove them easily, by storing a list of words that you consider to stop words.\n",
    "We are going to use NLTK (Natural Language Toolkit), NLTK has a list of stopwords stored in 16 different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           [happen, terribl, car, crash]\n",
       "1       [heard, earthquak, differ, citi, stay, safe, e...\n",
       "2       [forest, fire, spot, pond, gees, flee, across,...\n",
       "3                     [apocalyps, light, spokan, wildfir]\n",
       "4                [typhoon, soudelor, kill, china, taiwan]\n",
       "                              ...                        \n",
       "3258    [earthquak, safeti, los, angel, ûò, safeti, fa...\n",
       "3259    [storm, ri, wors, last, hurrican, citi, amp, h...\n",
       "3260    [green, line, derail, chicago, http, co, utbxl...\n",
       "3261    [meg, issu, hazard, weather, outlook, hwo, htt...\n",
       "3262    [cityofcalgari, activ, municip, emerg, plan, y...\n",
       "Name: cleaned, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df_train['cleaned'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df_test['cleaned'].apply(lambda x: [item for item in x if item not in stop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-convert text \"list\" to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our, deed, are, the, reason, of, this, earthqu...\n",
       "1              forest, fire, near, la, rong, sask, canada\n",
       "2       all, resid, ask, to, shelter, in, place, are, ...\n",
       "3       peopl, receiv, wildfir, evacu, order, in, cali...\n",
       "4       just, got, sent, this, photo, from, rubi, alas...\n",
       "                              ...                        \n",
       "7608    two, giant, crane, hold, a, bridg, collaps, in...\n",
       "7609    aria_ahrari, thetawniest, the, out, of, contro...\n",
       "7610    m, utc, km, s, of, volcano, hawaii, http, t, c...\n",
       "7611    polic, investig, after, an, e, bike, collid, w...\n",
       "7612    the, latest, more, home, raze, by, northern, c...\n",
       "Name: cleaned, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['cleaned'] = df_train['cleaned'].apply(', '.join)\n",
    "df_test['cleaned'] = df_test['cleaned'].apply(', '.join)\n",
    "df_train['cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Let's Visualize the Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word Cloud</h3>\n",
    "\n",
    "Data visualizations (like charts, graphs, infographics, and more) give a valuable way to visualize, and statistical important information, but what if your raw data is text-based? <br>\n",
    "    \n",
    "<span style=\"color:#F4D03F\">Word cloud</span> generators can help simplify this process.<br>\n",
    "    \n",
    "A word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it’s mentioned within a given text and the more important it is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vectorizers\n",
    "Now, We have nearly cleaned the data, \n",
    "But as we said before, Computers are not like humans, it cannot understand text, machines only capable of understanding numbers and performing numerical computation. <br>\n",
    "so our next step is to convert out text column to numeric data for the computer to be able to extract meaning from them. <br>\n",
    "\n",
    "This method called, vectorizing.<br>\n",
    "\n",
    "By vectorizing the documents we can further perform multiple tasks such as finding the relevant documents, ranking, clustering and so on.\n",
    "This is the same thing that happens when you perform a google search. The web pages are called documents and the search text with which you search is called a query. google maintains a fixed representation for all of the documents. When you search with a query, google will find the relevance of the query with all of the documents, ranks them in the order of relevance and shows you the top k documents, all of this process is done using the vectorized form of query and documents. Although Googles algorithms are highly sophisticated and optimized, this is their underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF stands for “Term Frequency — Inverse Document Frequency”. <br>\n",
    "This is a technique to quantify a word in documents,<br>\n",
    "we generally compute a weight to each word which signifies the importance of the word in the document and corpus.\n",
    "\n",
    "From our intuition, we think that the words which appear more often should have a greater weight in textual data analysis, <br>\n",
    "but that’s not always the case. <br>\n",
    "Words such as “the”, “will”, and “you” — called stopwords — appear the most in a corpus of text, but are of very little significance.  <br>\n",
    "Instead, the words which are rare are the ones that actually help in distinguishing between the data, and carry more weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF: Term Frequency\n",
    "\n",
    "This measures the frequency of a word in a document. <br>\n",
    "we divide the the frequency with the total number of words in the document.<br>\n",
    "\n",
    "## IDF: Inverse Data Frequency\n",
    "used to calculate the weight of rare words across all documents in the corpus. <br>\n",
    "The words that occur rarely in the corpus have a high IDF score.\n",
    "\n",
    "Combining these two we come up with the TF-IDF score (w) for a word in a document in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "tfidfvectorizer_train = TfidfVectorizer(\n",
    "                    analyzer='word',\n",
    "                    stop_words= 'english',\n",
    "                    ngram_range=(1, 1)\n",
    "                    )\n",
    "\n",
    "# convert th documents into a matrix\n",
    "# Train data set\n",
    "unigramdataGet_train = tfidfvectorizer_train.fit_transform(df_train[\"text\"].astype('str'))\n",
    "unigramdataGet_train = unigramdataGet_train.toarray()\n",
    "\n",
    "vocab_train = tfidfvectorizer_train.get_feature_names()\n",
    "unigramdata_features_train=pd.DataFrame(np.round(unigramdataGet_train, 1), columns=vocab_train)\n",
    "unigramdata_features_train[unigramdata_features_train>0] = 1\n",
    "\n",
    "\n",
    "# Test dataset\n",
    "unigramdataGet_test= tfidfvectorizer_train.fir_transform(df_test[\"text\"].astype('str'))\n",
    "unigramdataGet_test = unigramdataGet_test.toarray()\n",
    "\n",
    "vocab_test = tfidfvectorizer_train.get_feature_names()\n",
    "unigramdata_features_test=pd.DataFrame(np.round(unigramdataGet_test, 1), columns=vocab_test)\n",
    "unigramdata_features_test[unigramdata_features_test>0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = unigramdata_features_train\n",
    "y_train = df_train[\"target\"]\n",
    "\n",
    "x_test = unigramdata_features_test\n",
    "y_test = df_sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9556324732536186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "print(f1_score(model.predict(x_train),y_train))\n",
    "pred=model.predict(x_test)\n",
    "\n",
    "print('Accuracy= {:.3f}'.format(metrics.accuracy_score(model.predict(x_train),y_train)))\n",
    "\n",
    "# print('Precision',round(f1_score(y_test, y_pred),2),'%')\n",
    "\n",
    "# print('Recall',round(recall_score(y_test, y_pred),2),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.963\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy= {:.3f}'.format(metrics.accuracy_score(model.predict(x_train),y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.98 %\n"
     ]
    }
   ],
   "source": [
    "print('Recall',round(recall_score(model.predict(x_train),y_train),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We have nearly cleaned the data, \n",
    "But as we said before, Computers are not like humans, it cannot understand text, machines only capable of understanding numbers and performing numerical computation. <br>\n",
    "so our next step is to convert out text column to numeric data for the computer to be able to extract meaning from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
